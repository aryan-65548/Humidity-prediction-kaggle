{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('/mnt/user-data/uploads/train.csv')\n",
    "test = pd.read_csv('/mnt/user-data/uploads/test.csv')\n",
    "sample = pd.read_csv('/mnt/user-data/uploads/sample_submission.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nColumns: {train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(train.isnull().sum())\n",
    "print(f\"\\nTarget (Humidity) stats:\")\n",
    "print(train['Humidity'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train['Humidity'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Humidity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Humidity Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "train[['Temperature (C)', 'Apparent Temperature (C)', 'Wind Speed (km/h)', \n",
    "       'Visibility (km)', 'Pressure (millibars)']].boxplot()\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Feature Distributions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns\n",
    "correlations = train[numeric_cols].corr()['Humidity'].sort_values(ascending=False)\n",
    "print(\"Correlation with Humidity:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Create new features from existing ones\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Temperature difference\n",
    "    df['Temp_Diff'] = df['Temperature (C)'] - df['Apparent Temperature (C)']\n",
    "    \n",
    "    # Wind features\n",
    "    df['Wind_Speed_Sq'] = df['Wind Speed (km/h)'] ** 2\n",
    "    \n",
    "    # Cyclical encoding for wind direction\n",
    "    df['Wind_Sin'] = np.sin(np.radians(df['Wind Bearing (degrees)']))\n",
    "    df['Wind_Cos'] = np.cos(np.radians(df['Wind Bearing (degrees)']))\n",
    "    \n",
    "    # Pressure deviation from standard\n",
    "    df['Pressure_Dev'] = df['Pressure (millibars)'] - 1013.25\n",
    "    \n",
    "    # Visibility log transform\n",
    "    df['Visibility_Log'] = np.log1p(df['Visibility (km)'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train = engineer_features(train)\n",
    "test = engineer_features(test)\n",
    "\n",
    "print(\" Features engineered!\")\n",
    "print(f\"New features: Temp_Diff, Wind_Speed_Sq, Wind_Sin, Wind_Cos, Pressure_Dev, Visibility_Log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features only (exclude text columns and target)\n",
    "exclude = ['Idx', 'Humidity', 'Summary', 'Precip Type', 'Daily Summary']\n",
    "feature_cols = [col for col in train.columns if col not in exclude]\n",
    "\n",
    "print(f\"Selected {len(feature_cols)} features:\")\n",
    "for col in feature_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = train[feature_cols].copy()\n",
    "y = train['Humidity'].copy()\n",
    "X_test = test[feature_cols].copy()\n",
    "\n",
    "print(f\"\\nBefore cleaning:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"NaN in X: {X.isna().sum().sum()}\")\n",
    "print(f\"NaN in y: {y.isna().sum()}\")\n",
    "print(f\"NaN in X_test: {X_test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN values\n",
    "# Fill missing values with median\n",
    "X = X.fillna(X.median())\n",
    "X_test = X_test.fillna(X_test.median())\n",
    "\n",
    "# Remove any remaining NaN in features with 0\n",
    "X = X.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "# Handle target - drop rows with NaN in target\n",
    "valid_mask = ~y.isna()\n",
    "X = X[valid_mask]\n",
    "y = y[valid_mask]\n",
    "\n",
    "# Reset indices\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nAfter cleaning:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"NaN in X: {X.isna().sum().sum()}\")\n",
    "print(f\"NaN in y: {y.isna().sum()}\")\n",
    "print(f\"NaN in X_test: {X_test.isna().sum().sum()}\")\n",
    "print(f\"\\n Data ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_full_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\" Data scaled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.01, max_iter=5000),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'val_r2': val_r2,\n",
    "        'val_rmse': val_rmse\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"  Val R¬≤:   {val_r2:.4f}\")\n",
    "    print(f\"  Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "print(\"\\n All models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': name,\n",
    "        'Train R¬≤': results[name]['train_r2'],\n",
    "        'Val R¬≤': results[name]['val_r2'],\n",
    "        'Val RMSE': results[name]['val_rmse']\n",
    "    }\n",
    "    for name in results.keys()\n",
    "]).sort_values('Val R¬≤', ascending=False)\n",
    "\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_name = comparison.iloc[0]['Model']\n",
    "best_model = results[best_name]['model']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_name}\")\n",
    "print(f\"   Validation R¬≤: {results[best_name]['val_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_pos = np.arange(len(comparison))\n",
    "plt.bar(x_pos - 0.2, comparison['Train R¬≤'], 0.4, label='Train R¬≤', alpha=0.8)\n",
    "plt.bar(x_pos + 0.2, comparison['Val R¬≤'], 0.4, label='Val R¬≤', alpha=0.8)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x_pos, comparison['Model'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Final Training on Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain best model on full training data\n",
    "print(f\"Retraining {best_name} on full dataset...\")\n",
    "best_model.fit(X_full_scaled, y)\n",
    "print(\" Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "predictions = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Clip to valid range [0, 1]\n",
    "predictions = np.clip(predictions, 0, 1)\n",
    "\n",
    "print(f\"Predictions statistics:\")\n",
    "print(f\"  Min:  {predictions.min():.4f}\")\n",
    "print(f\"  Max:  {predictions.max():.4f}\")\n",
    "print(f\"  Mean: {predictions.mean():.4f}\")\n",
    "print(f\"  Std:  {predictions.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(predictions, bins=50, edgecolor='black', alpha=0.7, label='Test Predictions')\n",
    "plt.hist(y, bins=50, alpha=0.5, label='Train Target')\n",
    "plt.xlabel('Humidity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([y, predictions], labels=['Train', 'Test Pred'])\n",
    "plt.ylabel('Humidity')\n",
    "plt.title('Distribution Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'Idx': test['Idx'],\n",
    "    'Humidity': predictions\n",
    "})\n",
    "\n",
    "print(\"Submission preview:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nShape: {submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "output_path = '/mnt/user-data/outputs/submission.csv'\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {output_path}\")\n",
    "\n",
    "# Verify\n",
    "verify = pd.read_csv(output_path)\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  Rows: {len(verify)}\")\n",
    "print(f\"  Columns: {list(verify.columns)}\")\n",
    "print(f\"  First few rows:\")\n",
    "print(verify.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
